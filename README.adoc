:toc: macro

:repository: $(git config --get remote.origin.url)

= State-of-the-Art Kubernetes Playground

This guide walks you through setting up Istio on a Kubernetes cluster with:

* Automated A/B testing
* Canary releases
* GitOps pipelines.

'''

toc::[]

'''

== Architecture

image:https://raw.githubusercontent.com/weaveworks/flagger/master/docs/diagrams/flagger-gitops-istio.png[Progressive Delivery GitOps Pipeline]

Components and their responsibilities:

Istio (service mesh)::
Manages the traffic flows between microservices, enforcing access policies and aggregating telemetry data.

Prometheus (monitoring system)::
Time series database that collects and stores the service mesh metrics.

Flux (GitOps operator)::
Synchronizes YAMLs and Helm charts between Git and clusters. Scans container registries and deploys new images.

Helm Operator (CRD controller)::
Automates Helm chart releases.

Flagger (progressive delivery operator)::
Automates the release process using Istio routing for traffic shifting and Prometheus metrics for canary analysis.

== Setup

In order to run the playground you need to install appropriate software, configure your git repository and configure your cluster.
Most of these steps are automated.

=== Software setup

|===
|Name |Tested version |Installation

|Kubernetes cluster
|1.21.2
|Install https://github.com/kubernetes/minikube/releases[Minikube].

|Flux CLI
|1.21.2
|`brew install fluxctl`

|Helm
|3.5.2
|`brew install helm`

|===

=== Repository setup

You need to fork and clone this repository.

=== Cluster setup

Install Flux and its Helm Operator by specifying your fork URL:

[source,bash,subs="attributes"]
----
./scripts/flux-init.sh {repository}
----

At a startup, Flux generates an SSH key and logs the public key.
The above command will print the public key.

In order to sync your cluster state with git you need to copy the public key and create a deployment key with write
access on your GitHub repository. On GitHub go to _Settings &gt; Deploy keys_ click on _Add deploy key_,
check _Allow write access_, paste the Flux public key and click _Add key_.

Flux will do the following:

* install the Istio operator
* wait for Istio control plane to be ready
* install Flagger CRDs and Helm Releases
* create the Istio public gateway
* create the `prod` namespace
* create the load tester deployment
* create the frontend deployment and canary
* create the backend deployment and canary

image:https://raw.githubusercontent.com/fluxcd/helm-operator-get-started/master/diagrams/flux-istio-operator.png[Flux Istio Operator]

You can customize the Istio installation with the `IstioOperator` resource located at `istio/control-plane.yaml`.

[source,yaml]
----
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  namespace: istio-system
  name: istio-default
spec:
  profile: default
  components:
    pilot:
      k8s:
        resources:
          requests:
            cpu: 10m
            memory: 100Mi
----

After modifying the Istio settings, you can push the change to git and Flux will apply it on the cluster.
The Istio operator will reconfigure the Istio control plane according to your changes.
It can take a couple of minutes for Flux to sync and apply the changes, to speed up you can use `fluxctl sync` to trigger a git sync.

=== Workloads bootstrap

When Flux syncs the Git repository with your cluster, it creates the frontend/backend deployment, HPA and a canary object.
Flagger uses the canary definition to create a series of objects: Kubernetes deployments, ClusterIP services, Istio destination rules and virtual services.
These objects expose the application on the mesh and drive the canary analysis and promotion.

Check if Flagger has successfully initialized the canaries: 

----
kubectl -n prod get canaries

NAME       STATUS        WEIGHT
backend    Initialized   0
frontend   Initialized   0
----

When the `frontend-primary` deployment comes online,
Flagger will route all traffic to the primary pods and scale to zero the `frontend` deployment.

Normally, we would be able to find the Istio ingress gateway address with:
[source,bash]
----
kubectl -n istio-system get svc istio-ingressgateway -o json | jq .status.loadBalancer.ingress
----

*However*, as we are using `minikube` cluster no external IP is really available.
Therefore issue the following command. This will open several windows in your browser.
One of them will display the frontend web page.

----
minikube service istio-ingressgateway -n istio-system

|--------------|----------------------|-------------------|---------------------------|
|  NAMESPACE   |         NAME         |    TARGET PORT    |            URL            |
|--------------|----------------------|-------------------|---------------------------|
| istio-system | istio-ingressgateway | status-port/15021 | http://192.168.49.2:31214 |
|              |                      | http2/80          | http://192.168.49.2:32703 |
|              |                      | https/443         | http://192.168.49.2:30741 |
|              |                      | tls/15443         | http://192.168.49.2:30939 |
|--------------|----------------------|-------------------|---------------------------|
----

== Features

=== Canary releases

Flagger implements a control loop that gradually shifts traffic to the canary while measuring key performance indicators like HTTP requests success rate, requests average duration and pod health.
Based on analysis of the KPIs a canary is promoted or aborted.

A canary analysis is triggered by changes in any of the following objects:
* Deployment PodSpec (container image, command, ports, env, etc)
* ConfigMaps and Secrets mounted as volumes or mapped to environment variables

For workloads that are not receiving constant traffic Flagger can be configured with a webhook,
that when called, will start a load test for the target workload.

image:https://raw.githubusercontent.com/weaveworks/flagger/master/docs/diagrams/flagger-canary-steps.png[Flagger Canary Release]

Trigger a canary deployment for the backend app by updating the container image:

[source,bash]
----
$ export FLUX_FORWARD_NAMESPACE=flux

$ fluxctl release --workload=prod:deployment/backend \
--update-image=stefanprodan/podinfo:3.1.1

Submitting release ...
WORKLOAD                 STATUS   UPDATES
prod:deployment/backend  success  backend: stefanprodan/podinfo:3.1.0 -> 3.1.1
Commit pushed:	ccb4ae7
Commit applied:	ccb4ae7
----

Flagger detects that the deployment revision changed and starts a new rollout:

[source,bash]
----
$ kubectl -n prod describe canary backend

Events:

New revision detected! Scaling up backend.prod
Starting canary analysis for backend.prod
Pre-rollout check conformance-test passed
Advance backend.prod canary weight 5
...
Advance backend.prod canary weight 50
Copying backend.prod template spec to backend-primary.prod
Promotion completed! Scaling down backend.prod
----

During the analysis the canaryâ€™s progress can be monitored with Grafana. You can access the dashboard using port forwarding:

[source,bash]
----
kubectl -n istio-system port-forward svc/flagger-grafana 3000:80
----

The Istio dashboard URL is
http://localhost:3000/d/flagger-istio/istio-canary?refresh=10s&orgId=1&var-namespace=prod&var-primary=backend-primary&var-canary=backend

image:https://raw.githubusercontent.com/weaveworks/flagger/master/docs/screens/demo-backend-dashboard.png[Canary Deployment]

Note that if new changes are applied to the deployment during the canary analysis, Flagger will restart the analysis phase.

=== A/B testing

In addition to weighted routing, Flagger can be configured to route traffic to the canary based on HTTP match conditions.
In an A/B testing scenario, you'll be using HTTP headers or cookies to target a certain segment of your users.
This is particularly useful for frontend applications that require session affinity.

You can enable A/B testing by specifying the HTTP match conditions, and the number of iterations:

[source,yaml]
----
  analysis:
    # schedule interval (default 60s)
    interval: 10s
    # max number of failed metric checks before rollback
    threshold: 10
    # total number of iterations
    iterations: 12
    # canary match condition
    match:
      - headers:
          user-agent:
            regex: ".*Firefox.*"
      - headers:
          cookie:
            regex: "^(.*?;)?(type=insider)(;.*)?$"
----

The above configuration will run an analysis for two minutes targeting Firefox users and those that
have an insider cookie. The frontend configuration can be found at `prod/frontend/canary.yaml`.

Trigger a deployment by updating the frontend container image:

[source,bash]
----
$ fluxctl release --workload=prod:deployment/frontend \
--update-image=stefanprodan/podinfo:3.1.1
----

Flagger detects that the deployment revision changed and starts the A/B testing:

[source,bash]
----
$ kubectl -n istio-system logs deploy/flagger -f | jq .msg

New revision detected! Scaling up frontend.prod
Waiting for frontend.prod rollout to finish: 0 of 1 updated replicas are available
Pre-rollout check conformance-test passed
Advance frontend.prod canary iteration 1/10
...
Advance frontend.prod canary iteration 10/10
Copying frontend.prod template spec to frontend-primary.prod
Waiting for frontend-primary.prod rollout to finish: 1 of 2 updated replicas are available
Promotion completed! Scaling down frontend.prod
----

You can monitor all canaries with:

[source,bash]
----
$ watch kubectl get canaries --all-namespaces

NAMESPACE   NAME      STATUS        WEIGHT
prod        frontend  Progressing   100
prod        backend   Succeeded     0
----

=== Rollback based on Istio metrics

Flagger makes use of the metrics provided by Istio telemetry to validate the canary workload.
The frontend app https://github.com/stefanprodan/gitops-istio/blob/master/prod/frontend/canary.yaml[analysis]
defines two metric checks: 

[source,yaml]
----
    metrics:
      - name: error-rate
        templateRef:
          name: error-rate
          namespace: istio-system
        thresholdRange:
          max: 1
        interval: 30s
      - name: latency
        templateRef:
          name: latency
          namespace: istio-system
        thresholdRange:
          max: 500
        interval: 30s
----

The Prometheus queries used for checking the error rate and latency are located at
https://github.com/stefanprodan/gitops-istio/blob/master/flagger/istio-metrics.yaml[flagger/istio-metrics.yaml].

During the canary analysis you can generate HTTP 500 errors and high latency to test Flagger's rollback.

Generate HTTP 500 errors:

[source,bash]
----
watch curl -b 'type=insider' http://<INGRESS-IP>/status/500
----

Generate latency:

[source,bash]
----
watch curl -b 'type=insider' http://<INGRESS-IP>/delay/1
----

When the number of failed checks reaches the canary analysis threshold, the traffic is routed back to the primary,
the canary is scaled to zero and the rollout is marked as failed.

[source,text]
----
$ kubectl -n istio-system logs deploy/flagger -f | jq .msg

New revision detected! Scaling up frontend.prod
Pre-rollout check conformance-test passed
Advance frontend.prod canary iteration 1/10
Halt frontend.prod advancement error-rate 31 > 1
Halt frontend.prod advancement latency 2000 > 500
...
Rolling back frontend.prod failed checks threshold reached 10
Canary failed! Scaling down frontend.prod
----

You can extend the analysis with custom metric checks targeting
https://docs.flagger.app/usage/metrics#prometheus[Prometheus],
https://docs.flagger.app/usage/metrics#datadog[Datadog] and
https://docs.flagger.app/usage/metrics#amazon-cloudwatch[Amazon CloudWatch].

=== Alerting

Flagger can be configured to send Slack notifications.
You can enable alerting by adding the Slack settings to Flagger's Helm Release:

[source,yaml]
----
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: flagger
  namespace: istio-system
spec:
  values:
    slack:
      user: flagger
      channel: general
      url: https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK
----

Once configured with a Slack incoming *webhook*, Flagger will post messages when a canary deployment
has been initialised, when a new revision has been detected and if the canary analysis failed or succeeded.

image:https://raw.githubusercontent.com/weaveworks/flagger/master/docs/screens/slack-canary-notifications.png[Slack Notifications]

A canary deployment will be rolled back if the progress deadline exceeded or if the analysis reached the
maximum number of failed checks:

image:https://raw.githubusercontent.com/weaveworks/flagger/master/docs/screens/slack-canary-failed.png[Slack Notifications]

For configuring alerting at canary level for Slack, MS Teams, Discord or Rocket see the https://docs.flagger.app/usage/alerting#canary-configuration[docs].

=== Getting Help

If you have any questions about progressive delivery:

* Invite yourself to the https://slack.weave.works/[Weave community slack]
 and join the https://weave-community.slack.com/messages/flux/[#flux] and https://weave-community.slack.com/messages/flagger/[#flagger] channel.
* Join the https://www.meetup.com/pro/Weave/[Weave User Group] and get invited to online talks,
 hands-on training and meetups in your area.

Your feedback is always welcome!